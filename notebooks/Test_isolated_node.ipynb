{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0db1a244-36eb-49f4-9910-ff17a22f509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/sam/Documents/network/supernode/BREC_test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01a83adf-edbc-415c-b564-4daa437490d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from concepts.concepts import *\n",
    "from concepts.transformations import AddSupernodesHeteroMulti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e0a505cf-868b-45bf-a499-c74f7a8cb36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "\n",
    "class AddSupernodesHeteroMulti(BaseTransform):\n",
    "    def __init__(self, concepts_list) -> None:\n",
    "        self.concepts_list = concepts_list\n",
    "\n",
    "    def forward(self, data: Data) -> HeteroData:\n",
    "        data_with_supernodes = HeteroData({\n",
    "            'normal'    : {'x' : data.x.float()},\n",
    "            ('normal', 'orig', 'normal'  )   : { 'edge_index': data.edge_index, 'edge_attr' : data.edge_attr},\n",
    "#            ('normal', 'orig', 'normal'  )   : { 'edge_index': data.edge_index},\n",
    "        })\n",
    "        t1 = torch.arange(data.x.shape[0])\n",
    "        data_with_supernodes['normal', 'identity', 'normal'].edge_index = torch.stack([t1, t1], dim=0).long()\n",
    "\n",
    "        G = to_networkx(data, to_undirected=True, node_attrs=[\"x\"])\n",
    "\n",
    "        # find all the concepts in the graph on the original graph only\n",
    "        for concept in self.concepts_list:\n",
    "            concept_name = concept[\"name\"]\n",
    "            comp = concept[\"fun\"](G, *concept[\"args\"])\n",
    "            if len(comp) != 0:\n",
    "                current_supernode = 0\n",
    "                from_normal = []\n",
    "                to_sup      = []\n",
    "                supnodes    = []\n",
    "                for concept in comp:\n",
    "                    supnodes.append(current_supernode)\n",
    "                    for node in concept:\n",
    "                        from_normal.append(node)\n",
    "                        to_sup.append(current_supernode)\n",
    "                    current_supernode += 1\n",
    "\n",
    "                toSup_edges = torch.Tensor((from_normal, to_sup)).long()\n",
    "                toNor_edges = torch.Tensor((to_sup, from_normal)).long()\n",
    "                #data_with_supernodes[concept_name].x = torch.zeros(len(comp), data.num_features)\n",
    "                data_with_supernodes[concept_name].x = torch.ones(len(comp), data.num_features)\n",
    "                data_with_supernodes['normal', 'toSup', concept_name].edge_index = toSup_edges\n",
    "                data_with_supernodes[concept_name, 'toNor', 'normal'].edge_index = toNor_edges\n",
    "                t2 = torch.arange(len(comp))\n",
    "                data_with_supernodes[concept_name, 'identity', concept_name].edge_index = torch.stack([t2, t2], dim=0).long()\n",
    "            else:\n",
    "                data_with_supernodes[concept_name].x = torch.zeros(1, data.num_features)\n",
    "\n",
    "        return data_with_supernodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e9dc99f3-2ef1-472f-b916-a92093b23f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MLP, global_add_pool, HeteroConv, SimpleConv, GATConv, GINConv\n",
    "from torch_geometric.nn import HGTConv, Linear\n",
    "\n",
    "def get_HGAT_multi_simple(args, supnodes_name, dropout=0.5, hidden_channels=64,\n",
    "                   num_layers=4, out_channels=16):\n",
    "    SConv_dict = {\n",
    "            ('normal', 'identity', 'normal'): SimpleConv('add'),\n",
    "            }\n",
    "    for supnode_type in supnodes_name:\n",
    "        SConv_dict |= {('normal', 'toSup', supnode_type) : SimpleConv('add')}\n",
    "\n",
    "\n",
    "    SConv = HeteroConv(SConv_dict, aggr='sum')\n",
    "\n",
    "    HConvs = torch.nn.ModuleList()\n",
    "    for _ in range(num_layers):\n",
    "        Conv_dict = {(\"normal\", \"orig\", \"normal\") : GATConv((-1, -1), hidden_channels, add_self_loops=True)}\n",
    "\n",
    "        for supnode_type in supnodes_name:\n",
    "            Conv_dict |= {(\"normal\", \"toSup\", supnode_type) : SimpleConv('add'),\n",
    "                          (supnode_type, \"toNor\", \"normal\") : GATConv((-1, -1), hidden_channels, add_self_loops=False)}\n",
    "\n",
    "        conv = HeteroConv(Conv_dict, aggr='sum')\n",
    "        HConvs.append(conv)\n",
    "\n",
    "    class HGAT_simple_multi(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(HGAT_simple_multi, self).__init__()\n",
    "            self.supinit = SConv\n",
    "            self.convs = HConvs\n",
    "            self.readout = global_add_pool\n",
    "            self.classifier = MLP([hidden_channels, hidden_channels, out_channels],\n",
    "                                   norm=\"batch_norm\", dropout=dropout)\n",
    "\n",
    "        def forward(self, data):\n",
    "            x_dict, edge_index_dict = (data.x_dict, data.edge_index_dict)\n",
    "            x_dict = self.supinit(x_dict, edge_index_dict)\n",
    "\n",
    "            print(\"here\", x_dict.keys())\n",
    "            for conv in self.convs:\n",
    "                x_dict = conv(x_dict, edge_index_dict)\n",
    "                x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "\n",
    "            return x_dict\n",
    "\n",
    "\n",
    "    model = HGAT_simple_multi()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4079182-f892-426c-bc66-2dd18a28440b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cliques found:  0\n",
      "HeteroDataBatch(\n",
      "  normal={\n",
      "    x=[17, 7],\n",
      "    batch=[17],\n",
      "    ptr=[2],\n",
      "  },\n",
      "  GCB={\n",
      "    x=[3, 7],\n",
      "    batch=[3],\n",
      "    ptr=[2],\n",
      "  },\n",
      "  GMC={\n",
      "    x=[1, 7],\n",
      "    batch=[1],\n",
      "    ptr=[2],\n",
      "  },\n",
      "  GLP2={\n",
      "    x=[4, 7],\n",
      "    batch=[4],\n",
      "    ptr=[2],\n",
      "  },\n",
      "  (normal, orig, normal)={\n",
      "    edge_index=[2, 38],\n",
      "    edge_attr=[38, 4],\n",
      "  },\n",
      "  (normal, identity, normal)={ edge_index=[2, 17] },\n",
      "  (normal, toSup, GCB)={ edge_index=[2, 18] },\n",
      "  (GCB, toNor, normal)={ edge_index=[2, 18] },\n",
      "  (GCB, identity, GCB)={ edge_index=[2, 3] },\n",
      "  (normal, toSup, GLP2)={ edge_index=[2, 17] },\n",
      "  (GLP2, toNor, normal)={ edge_index=[2, 17] },\n",
      "  (GLP2, identity, GLP2)={ edge_index=[2, 4] }\n",
      ")\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0.]])\n",
      "dict_keys(['normal', 'GCB', 'GMC', 'GLP2'])\n",
      "here dict_keys(['normal', 'GCB', 'GLP2'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'normal': tensor([[14.2489,  0.0000,  8.4846,  ...,  7.1229,  0.0000,  0.0000],\n",
       "         [14.2490,  0.0000,  8.4907,  ...,  7.1360,  0.0000,  0.0000],\n",
       "         [14.1447,  0.0000,  8.3263,  ...,  6.9013,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.3303,  0.0000,  0.2025,  ...,  0.6132,  0.5468,  0.0000],\n",
       "         [ 0.2340,  0.0806,  0.1751,  ...,  0.5406,  0.3612,  0.0000],\n",
       "         [ 0.2340,  0.0806,  0.1751,  ...,  0.5406,  0.3612,  0.0000]],\n",
       "        grad_fn=<ReluBackward0>),\n",
       " 'GCB': tensor([[4.1627e+01, 0.0000e+00, 2.3845e+01, 1.6031e+01, 3.4333e+01, 0.0000e+00,\n",
       "          6.9825e+01, 1.3890e+00, 1.0975e+01, 5.0513e+00, 3.3483e+01, 0.0000e+00,\n",
       "          1.3676e+01, 0.0000e+00, 0.0000e+00, 1.3857e+00, 0.0000e+00, 1.9050e+01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5465e+01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.8609e+00, 3.8395e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          5.5033e-01, 2.6717e+01, 0.0000e+00, 7.6281e+00, 9.0239e+01, 0.0000e+00,\n",
       "          0.0000e+00, 5.5879e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1562e+01,\n",
       "          1.8623e+01, 2.1070e-01, 0.0000e+00, 0.0000e+00, 1.5850e+01, 0.0000e+00,\n",
       "          7.1049e+00, 1.9423e+01, 0.0000e+00, 6.8012e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 3.4439e+01, 1.7884e+01, 1.3620e+01, 3.1410e+01, 0.0000e+00,\n",
       "          8.8806e+00, 4.1682e+01, 1.1262e+01, 0.0000e+00],\n",
       "         [5.1840e+01, 0.0000e+00, 2.8518e+01, 1.8147e+01, 4.2264e+01, 0.0000e+00,\n",
       "          6.1670e+01, 2.1310e+00, 1.9483e+01, 6.7288e+00, 3.1625e+01, 0.0000e+00,\n",
       "          2.0864e+01, 3.3582e+00, 3.7745e+00, 0.0000e+00, 0.0000e+00, 1.7603e+01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4571e+01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 8.2633e+00, 3.4518e-01, 0.0000e+00, 0.0000e+00, 2.9850e+00,\n",
       "          5.1315e-02, 4.2249e+01, 0.0000e+00, 5.4211e+00, 9.2256e+01, 0.0000e+00,\n",
       "          0.0000e+00, 5.8014e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5737e+01,\n",
       "          1.6850e+01, 0.0000e+00, 0.0000e+00, 1.5959e+00, 1.9377e+01, 0.0000e+00,\n",
       "          1.3574e+01, 2.7259e+01, 0.0000e+00, 7.1803e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 4.1850e+01, 2.2979e+01, 1.0656e+01, 4.0776e+01, 0.0000e+00,\n",
       "          9.4898e+00, 4.5189e+01, 7.2942e+00, 0.0000e+00],\n",
       "         [6.8873e+01, 0.0000e+00, 3.4387e+01, 2.2259e+01, 5.4657e+01, 0.0000e+00,\n",
       "          4.8164e+01, 2.1326e+00, 3.3624e+01, 9.4589e+00, 2.9499e+01, 0.0000e+00,\n",
       "          3.2416e+01, 8.8624e+00, 9.7836e+00, 0.0000e+00, 0.0000e+00, 1.3910e+01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8805e+01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.9207e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.2715e+00,\n",
       "          0.0000e+00, 6.6707e+01, 0.0000e+00, 1.6578e+00, 9.5378e+01, 0.0000e+00,\n",
       "          0.0000e+00, 5.9574e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6535e+01,\n",
       "          1.4405e+01, 0.0000e+00, 0.0000e+00, 2.4257e+00, 2.4614e+01, 0.0000e+00,\n",
       "          2.4656e+01, 4.0589e+01, 0.0000e+00, 0.0000e+00, 2.7087e+00, 0.0000e+00,\n",
       "          0.0000e+00, 5.3138e+01, 3.2152e+01, 6.0530e+00, 5.5283e+01, 0.0000e+00,\n",
       "          1.0374e+01, 4.9949e+01, 4.6148e-03, 0.0000e+00]],\n",
       "        grad_fn=<ReluBackward0>),\n",
       " 'GLP2': tensor([[2.8505e+01, 0.0000e+00, 1.5525e+01, 1.1083e+01, 2.3573e+01, 0.0000e+00,\n",
       "          4.5724e+01, 6.0571e-01, 8.0680e+00, 3.6901e+00, 2.2042e+01, 0.0000e+00,\n",
       "          1.0144e+01, 0.0000e+00, 0.0000e+00, 4.0877e-01, 0.0000e+00, 1.2649e+01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1058e+01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.4778e+00, 2.5779e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          5.5033e-01, 1.9297e+01, 0.0000e+00, 5.3399e+00, 6.0511e+01, 0.0000e+00,\n",
       "          0.0000e+00, 3.6625e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4927e+01,\n",
       "          1.2587e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1049e+01, 0.0000e+00,\n",
       "          4.9724e+00, 1.4462e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.3454e+01, 1.2037e+01, 9.0547e+00, 2.1364e+01, 0.0000e+00,\n",
       "          5.9703e+00, 2.8278e+01, 6.6158e+00, 0.0000e+00],\n",
       "         [6.8873e+01, 0.0000e+00, 3.4387e+01, 2.2259e+01, 5.4657e+01, 0.0000e+00,\n",
       "          4.8164e+01, 2.1326e+00, 3.3624e+01, 9.4589e+00, 2.9499e+01, 0.0000e+00,\n",
       "          3.2416e+01, 8.8624e+00, 9.7836e+00, 0.0000e+00, 0.0000e+00, 1.3910e+01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8805e+01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.9207e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.2715e+00,\n",
       "          0.0000e+00, 6.6707e+01, 0.0000e+00, 1.6578e+00, 9.5378e+01, 0.0000e+00,\n",
       "          0.0000e+00, 5.9574e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6535e+01,\n",
       "          1.4405e+01, 0.0000e+00, 0.0000e+00, 2.4257e+00, 2.4614e+01, 0.0000e+00,\n",
       "          2.4656e+01, 4.0589e+01, 0.0000e+00, 0.0000e+00, 2.7087e+00, 0.0000e+00,\n",
       "          0.0000e+00, 5.3138e+01, 3.2152e+01, 6.0530e+00, 5.5283e+01, 0.0000e+00,\n",
       "          1.0374e+01, 4.9949e+01, 4.6148e-03, 0.0000e+00],\n",
       "         [2.0533e+01, 0.0000e+00, 1.1985e+01, 7.7101e+00, 1.6285e+01, 0.0000e+00,\n",
       "          3.5778e+01, 7.8332e-01, 5.0000e+00, 2.2537e+00, 1.7207e+01, 0.0000e+00,\n",
       "          6.1206e+00, 0.0000e+00, 0.0000e+00, 1.1576e+00, 0.0000e+00, 9.6044e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 6.9734e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 7.3939e-01, 2.2377e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.1744e+01, 0.0000e+00, 3.3809e+00, 4.4817e+01, 0.0000e+00,\n",
       "          0.0000e+00, 2.8012e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4960e+01,\n",
       "          9.3474e+00, 2.1070e-01, 0.0000e+00, 0.0000e+00, 7.4187e+00, 0.0000e+00,\n",
       "          3.1565e+00, 8.5330e+00, 0.0000e+00, 6.8012e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.6481e+01, 9.0725e+00, 6.9143e+00, 1.4987e+01, 0.0000e+00,\n",
       "          4.5801e+00, 2.0541e+01, 6.3555e+00, 0.0000e+00],\n",
       "         [3.3306e+01, 0.0000e+00, 1.9066e+01, 1.1731e+01, 2.7144e+01, 0.0000e+00,\n",
       "          4.2541e+01, 1.6034e+00, 1.2011e+01, 4.1090e+00, 2.1226e+01, 0.0000e+00,\n",
       "          1.2877e+01, 1.6393e+00, 1.8558e+00, 0.0000e+00, 0.0000e+00, 1.2264e+01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5085e+01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 4.5581e+00, 1.7732e-01, 0.0000e+00, 0.0000e+00, 1.4759e+00,\n",
       "          0.0000e+00, 2.6064e+01, 0.0000e+00, 3.5349e+00, 6.1208e+01, 0.0000e+00,\n",
       "          0.0000e+00, 3.9200e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2772e+01,\n",
       "          1.1355e+01, 0.0000e+00, 0.0000e+00, 7.6817e-01, 1.2567e+01, 0.0000e+00,\n",
       "          8.2951e+00, 1.6632e+01, 0.0000e+00, 7.1803e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.7104e+01, 1.4762e+01, 7.4283e+00, 2.6350e+01, 0.0000e+00,\n",
       "          6.3409e+00, 2.9910e+01, 5.5935e+00, 0.0000e+00]],\n",
       "        grad_fn=<ReluBackward0>)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "dataset = TUDataset(root=\"./dataset/TUD\", name='MUTAG')\n",
    "\n",
    "concepts_list_ex = [\n",
    "       {\"name\": \"GCB\", \"fun\": cycle_basis, \"args\": []},\n",
    "       {\"name\": \"GMC\", \"fun\": max_cliques, \"args\": []},\n",
    "       {\"name\": \"GLP2\", \"fun\": line_paths, \"args\": []}\n",
    "    ]\n",
    "\n",
    "print(\"cliques found: \", len(max_cliques(to_networkx(dataset[0], to_undirected=True))))\n",
    "\n",
    "supnodes_name = [concept['name'] for concept in concepts_list_ex]\n",
    "datasetT = dataset.transform = AddSupernodesHeteroMulti(concepts_list_ex)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1)\n",
    "data = next(iter(loader))\n",
    "print(data)\n",
    "print(data['GMC'].x)\n",
    "model = get_HGAT_multi_simple(None, supnodes_name)\n",
    "\n",
    "print(data.x_dict.keys())\n",
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d641940-8de5-4d54-bbd5-d410c536d002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c49a7f8-bc08-4c7f-857c-8178fe399d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_HGIN_multi_simple(args, supnodes_name, dropout=0.5, hidden_channels=64,\n",
    "                   num_layers=4, out_channels=16):\n",
    "    SConv_dict = {\n",
    "            ('normal', 'identity', 'normal'): SimpleConv('add'),\n",
    "            }\n",
    "    for supnode_type in supnodes_name:\n",
    "        SConv_dict |= {('normal', 'toSup', supnode_type) : SimpleConv('add')}\n",
    "\n",
    "\n",
    "    SConv = HeteroConv(SConv_dict, aggr='sum')\n",
    "\n",
    "    HConvs = torch.nn.ModuleList()\n",
    "    for _ in range(num_layers):\n",
    "        Conv_dict = {(\"normal\", \"orig\", \"normal\") :  GINConv(MLP([-1, hidden_channels, hidden_channels]))}\n",
    "\n",
    "        for supnode_type in supnodes_name:\n",
    "            Conv_dict |= {(\"normal\", \"toSup\", supnode_type) : SimpleConv('add'),\n",
    "                          (supnode_type, \"toNor\", \"normal\") : GINConv(MLP([-1, hidden_channels, hidden_channels]))}\n",
    "\n",
    "        conv = HeteroConv(Conv_dict, aggr='sum')\n",
    "        HConvs.append(conv)\n",
    "\n",
    "    class HGIN_simple_multi(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(HGIN_simple_multi, self).__init__()\n",
    "            self.supinit = SConv\n",
    "            self.convs = HConvs\n",
    "            self.readout = global_add_pool\n",
    "            self.classifier = MLP([hidden_channels, hidden_channels, out_channels],\n",
    "                                   norm=\"batch_norm\", dropout=dropout)\n",
    "\n",
    "        def forward(self, data):\n",
    "            x_dict, edge_index_dict, batch_dict = (data.x_dict, data.edge_index_dict, data.collect('batch'))\n",
    "            x_dict = self.supinit(x_dict, edge_index_dict)\n",
    "\n",
    "            for conv in self.convs:\n",
    "                x_dict = conv(x_dict, edge_index_dict)\n",
    "                x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "            \n",
    "            return x_dict\n",
    "\n",
    "    model = HGIN_simple_multi()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0156766a-774e-4646-999e-3c7dcc2e0d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cliques found:  0\n",
      "HeteroDataBatch(\n",
      "  normal={\n",
      "    x=[17, 7],\n",
      "    batch=[17],\n",
      "    ptr=[2],\n",
      "  },\n",
      "  GCB={\n",
      "    x=[3, 7],\n",
      "    batch=[3],\n",
      "    ptr=[2],\n",
      "  },\n",
      "  GMC={\n",
      "    x=[1, 7],\n",
      "    batch=[1],\n",
      "    ptr=[2],\n",
      "  },\n",
      "  GLP2={\n",
      "    x=[4, 7],\n",
      "    batch=[4],\n",
      "    ptr=[2],\n",
      "  },\n",
      "  (normal, orig, normal)={\n",
      "    edge_index=[2, 38],\n",
      "    edge_attr=[38, 4],\n",
      "  },\n",
      "  (normal, identity, normal)={ edge_index=[2, 17] },\n",
      "  (normal, toSup, GCB)={ edge_index=[2, 18] },\n",
      "  (GCB, toNor, normal)={ edge_index=[2, 18] },\n",
      "  (GCB, identity, GCB)={ edge_index=[2, 3] },\n",
      "  (normal, toSup, GLP2)={ edge_index=[2, 17] },\n",
      "  (GLP2, toNor, normal)={ edge_index=[2, 17] },\n",
      "  (GLP2, identity, GLP2)={ edge_index=[2, 4] }\n",
      ")\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0.]])\n",
      "dict_keys(['normal', 'GCB', 'GMC', 'GLP2'])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m get_HGIN_multi_simple(\u001b[38;5;28;01mNone\u001b[39;00m, supnodes_name)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mx_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 25\u001b[0m model(data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[48], line 37\u001b[0m, in \u001b[0;36mget_HGIN_multi_simple.<locals>.HGIN_simple_multi.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     34\u001b[0m x_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupinit(x_dict, edge_index_dict)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[0;32m---> 37\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m conv(x_dict, edge_index_dict)\n\u001b[1;32m     38\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m {key: x\u001b[38;5;241m.\u001b[39mrelu() \u001b[38;5;28;01mfor\u001b[39;00m key, x \u001b[38;5;129;01min\u001b[39;00m x_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_dict\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/conv/hetero_conv.py:159\u001b[0m, in \u001b[0;36mHeteroConv.forward\u001b[0;34m(self, *args_dict, **kwargs_dict)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_edge_level_arg:\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[edge_type](\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m     out_dict[dst]\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m out_dict\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/conv/gin_conv.py:84\u001b[0m, in \u001b[0;36mGINConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m     82\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps) \u001b[38;5;241m*\u001b[39m x_r\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn(out)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "dataset = TUDataset(root=\"./dataset/TUD\", name='MUTAG')\n",
    "\n",
    "concepts_list_ex = [\n",
    "       {\"name\": \"GCB\", \"fun\": cycle_basis, \"args\": []},\n",
    "       {\"name\": \"GMC\", \"fun\": max_cliques, \"args\": []},\n",
    "       {\"name\": \"GLP2\", \"fun\": line_paths, \"args\": []}\n",
    "    ]\n",
    "\n",
    "print(\"cliques found: \", len(max_cliques(to_networkx(dataset[0], to_undirected=True))))\n",
    "\n",
    "supnodes_name = [concept['name'] for concept in concepts_list_ex]\n",
    "datasetT = dataset.transform = AddSupernodesHeteroMulti(concepts_list_ex)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1)\n",
    "data = next(iter(loader))\n",
    "print(data)\n",
    "print(data['GMC'].x)\n",
    "model = get_HGIN_multi_simple(None, supnodes_name)\n",
    "\n",
    "print(data.x_dict.keys())\n",
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0bb64d-8e23-467d-9911-b9b2e2110e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
